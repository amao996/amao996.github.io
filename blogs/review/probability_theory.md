其他大佬整理的：

https://zhuanlan.zhihu.com/p/421160574

https://www.cnblogs.com/WYJdesigner/articles/17590639.html

### 先验概率&后验概率

P(A|B)是**已知B发生后A的条件概率**，也由于得自B的取值而被称作**A的后验概率，posterior probability**。posterior的意思是：coming after in time，即在之后发生的。我们的关注目标是A，且A是在B发生之后发生的，所以P(A|B)叫做A的posterior probability。

P(A)是**A的先验概率，prior probability**。prior的意思是：happening or existing before sth else。先发生，所以叫prior probability，即A先发生，它不考虑任何B方面的因素。

贝叶斯法则：
$$
P ( A _ { i } | B ) = \frac { P ( B | A _ { i } ) P ( A _ { i } ) } { \sum _ { j } P ( B | A _ { j } ) P ( A _ { j } ) }
$$
其中$$P ( B | A _ { i } )$$有时被称作**似然度。**贝叶斯法则可以概括为：**后验概率 = (似然度 \* 先验概率)/标准化常量**，也就是说，**后验概率与先验概率和似然度的乘积成正比**。

**贝叶斯分类法的基本思想**的是把计算后验概率的问题转化为计算先验概率的问题

### 置信区间&可信区间

贝叶斯学派认为参数真值不是固定的，是一个随机变量，而观察到的数据是固定的；频率学派认为参数真值是固定的且为未知的常数，观察到的数据是随机的。两者在数值可能是一样的，但含义却不一样。
（1）贝叶斯学派认为参数真值不是固定的，是一个随机变量，因此对于一个给定的贝叶斯95%**可信区间（credibility interval）**，代表的意思是真值有95%的可能落在这个区间范围内。
（2）频率学派认为，参数真值要么在这个区间内，要么不在，也就是说参数真值在这个区间内或不在区间范围，反别记为1或0。假设我们在上帝视角已知真值，多次重复计算95%置信区间（confidence interval）（假设算了100次），我们有把握在这100个的置信区间包含我们“已知真值”的区间有95个左右（也就是有约95%的置信区间包含“真值”）。
  **贝叶斯可信区间**与**频率置信区间**的区别主要是由两个学派对参数真值的看法不同：贝叶斯学派认为参数真值是一个随机变量，频率学派认为参数真值是一个未知的常数。而这也是贝叶斯学派和频率学派最本质上的不同。

### [贝叶斯定理](https://www.zhihu.com/question/21134457)

$$
\pi ( \theta _ { i } | x ) = \frac { f ( x | \theta _ { i } ) \pi ( \theta _ { i } ) } { \sum _ { i } f ( x | \theta _ { i } ) \pi ( \theta _ { i } ) }
$$

$$
\pi ( \theta | x ) = \frac { f ( x | \theta ) \pi ( \theta ) } { \int _ { \theta } f ( x | \theta ) \pi ( \theta ) d \theta }
$$

其中$$\pi$$指的是参数的概率分布，$$\pi (\theta)$$指的是先验概率，$$\pi ( \theta | x )$$指的是后验概率，$$f ( x | \theta )$$指的是我们观测到的样本的分布，也就是似然函数，其中积分求的区间是参数$$\theta$$所有可能取到的值的域，所以可以看出后验概率是在知道$$x$$的前提下在该域内的一个关于$$\theta$$的概率密度分布，每个$$\theta$$都有一个对应的可能性（也就是概率）。

### 期望&方差&协方差

期望：就是算概率均值

离散型随机变量：$$ E ( X ) = \sum _ { i = 1 } ^ { n } x _ { i } p _ { i }$$

连续性随机变量：$$ E ( X ) = \int _ { - \infty } ^ { \infty } x f ( x ) d x$$

方差：用来描述样本之间的差异，差异越大方差越大。可以记作平方的期望减去期望的平方。

离散型随机变量：$$ D ( X ) = \sum _ { i = 1 } ^ { n } \left[ x _ { i } - E ( X ) \right] ^ { 2 } p _ { i }$$

连续性随机变量：$$ D ( X ) = \int _ { - \infty } ^ { \infty } \left[ x - E ( X ) \right] ^ { 2 } f ( x ) d x$$

协方差：表示两个变量之间的变化趋势是否一致，表示为$$ C o v ( X , Y ) = [ X - E ( X ) ] [ Y - E ( Y ) ]$$，也就是X和Y分别与它的期望的差的乘积。

关联系数：协方差除以X和Y的标准差，$$ \rho = \frac { C o v ( X , Y ) } { \sqrt { D ( X ) } \sqrt { D ( Y ) } }$$

### 全概率公式

如果一个事件可以由n个子事件构成，这些子事件两两互不相容，和为全集，则对于任一事件B的概率可以表示为在每个事件 (A_i) 发生的条件下，事件 B 发生的概率的加权平均。

### 最大似然估计

概率：结果没有产生之前，根据环境参数，预测某件事情发生的可能性

似然：在确定的结果下，去推测产生这个结果的可能环境参数

利用已知的样本结果反推最有可能导致这种结果的参数值。（通过样本数据推断总体特征）

最大似然估计：观测样本集的似然函数取得最大值时的参数的值作为参数估计值，也就是
$$
L ( \theta ; D ) = P ( D ; \theta ) = \prod _ { i = 1 } ^ { N } P ( x _ { i } ; \theta )
$$
最优的$$\theta$$值是令观测样本发生概率最大的值，也就是令似然函数取值最大，参数的最大似然估计值可以写做
$$
\widehat { \theta } _ { M L } = a r g _ { \theta } \max L ( \theta ; D ) = a r g \max \prod _ { i = 1 } ^ { N } P ( x _ { i } ; \theta )
$$

一般来说，我们会对似然函数取log以将连乘变成累加，主要有两个目的：防止溢出和方便求导

### 独立性&互斥性

独立：事件之间互不影响，条件概率等于概率

互斥（互不相容）：不能同时发生

独立一定不线性相关(协方差为0)，不相关不一定独立

### [大数定律&中心极限定理](https://www.cnblogs.com/ofnoname/p/17050943.html)

大数定律：当样本数据无限大时，样本均值趋于总体均值。事件 A 发生的频率逼近于它的概率。切比雪夫（样本均值→总体均值），伯努利（频率→概率），辛钦（样本均值→期望）。

当X服从0-1分布时，辛钦大数定律就是伯努利大数定律。

切比雪夫大数定律没要求独立同分布，辛钦大数定律要求独立同分布。

中心极限定理：无论什么分布，当样本量 n 逐渐趋于无穷大时，n 个抽样样本的均值的频数，逐渐趋于正态分布。

### [熵](https://murphypei.github.io/blog/2019/12/entropy)

描述的是个事件的不确定性，如果某个事件有 n 个结果，每个结果的概率为 pn。那么这个事件的熵定义为$$ H ( p ) = - \sum _ { i = 1 } ^ { n } p _ { i } \log _ { 2 } p _ { i }$$

### 概率分布函数&概率密度函数

概率分布函数：就是把概率函数累加，$$ F ( x ) = P ( X \leq x ) = \sum _ { x _ { k } \leq x } p _ { k }$$

概率密度函数（连续型随机变量的概率函数）：概率密度函数是分布函数的导函数。某点的概率密度函数即为概率在该点的变化率(或导数)。